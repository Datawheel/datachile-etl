[
["index.html", "Data Chile Documentation What is Economic Complexity?", " Data Chile Documentation Pachá (Mauricio Vargas Sepúlveda) 2018-03-30 What is Economic Complexity? We owe to Adam Smith the idea that the division (specialization) of labor is the secret of the wealth of nations. In a modern interpretation, the division of labor into markets and organizations is what allows the knowledge held by few to reach many, making us collectively wiser. The complexity of an economy is related to the multiplicity of useful knowledge embedded in it. Because individuals are limited in what they know, the only way societies can expand their knowledge base is by facilitating the interaction of individuals in increasingly complex networks in order to make products. We can measure economic complexity by the mix of these products that countries are able to make. Some products, like medical imaging devices or jet engines, embed large amounts of knowledge and are the results of very large networks of people and organizations. These products cannot be made in simpler economies that are missing parts of this network’s capability set. Economic complexity, therefore, is expressed in the composition of a country’s productive output and reflects the structures that emerge to hold and combine knowledge. Volver al índice "],
["documentacion-de-datasets.html", "Section 1 Documentación de Datasets", " Section 1 Documentación de Datasets Volver al índice "],
["sobre-db.html", "Section 2 Sobre db", " Section 2 Sobre db "],
["datachile-etl-documentacion-general.html", "Section 3 Datachile ETL documentación general 3.1 El proceso 3.2 Documentación relacionada sobre ETL", " Section 3 Datachile ETL documentación general 3.1 El proceso Conseguimos información pública la cual cumple con todos los criterios de secreto estadístico. Ordenamos la información disponible de general a particular para ir completando los perfiles de comuna, región u país. Para procesar los datos usamos R y para subir los dato usamos Python. Se consigue la información que se espera integrar en el sitio y se los considera dataset candidato. Es importante que sea considerada data pública y anónimizada. Se utiliza R y los conceptos de Tidy Data para ordenar, validar e integrar la información. El resultado de este proceso son archivos CSV. Uno para los hechos (facts) y uno para cada dimensión relacionada (dims). Se debe considerar que existen dimensiones comunes como: geográficas (comunas, regiones), sexo, ISIC, etc. Se utilizaron scripts de iPython Notebook para validar e ingestar los archivos CSV. Importante mencionar que los scripts realizan la carga inicial, es decir que pisan la tabla con la nueva data, no la actualizan, por lo que es importante contar con los archivos históricos para poder actualizar o bien modificar los scripts. Si se trata de un dataset nuevo, una vez que existe la tabla en la base de datos se debe mapear al esquema como un cubo nuevo. Para más información sobre la configuración de API y esquema de cubos ver API Mondrian para DataChile. 3.2 Documentación relacionada sobre ETL Tidy Data R scripts iPython Notebook scripts Esquemas + DB Datasets Volver al índice "],
["sobre-python.html", "Section 4 Sobre Python", " Section 4 Sobre Python Volver al índice "],
["sobre-r.html", "Section 5 Sobre R", " Section 5 Sobre R Volver al índice "],
["sobre-tidy-data.html", "Section 6 Sobre Tidy Data", " Section 6 Sobre Tidy Data Tidy Data Process "]
]
